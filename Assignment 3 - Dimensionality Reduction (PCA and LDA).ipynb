{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-VDIG4gD0u7"
   },
   "source": [
    "## CS156 Assignment 3: Dimensionality Reduction\n",
    "_Yoav Rabinovich, Oct 2018_\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QpJ25Ya9gwJ"
   },
   "outputs": [],
   "source": [
    "#Collaborated on original preclass work with Michelle and Kalia #Michaliav\n",
    "from os import listdir, stat\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIzqgRsa92WK"
   },
   "outputs": [],
   "source": [
    "def img_to_array(img,size):\n",
    "    img = img.resize(size)\n",
    "    img_arr = np.frombuffer(img.tobytes(), dtype=np.uint8)\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzjb920v97dD"
   },
   "outputs": [],
   "source": [
    "resizing=(60,30)\n",
    "#extract jerseys with white background to maximize similarity\n",
    "jerseysdir = listdir(\"51ShirtsJerseys/Jerseys/\")\n",
    "jerseys = []\n",
    "#store the exact file sizes for quick duplicate removal without hashing\n",
    "visited = []\n",
    "for image in jerseysdir:\n",
    "    imagepath = \"51ShirtsJerseys/Jerseys/\" + image\n",
    "    img = Image.open(imagepath)\n",
    "    if (img.getpixel((0,0)) == (255,255,255)):\n",
    "        jerseys.append(img_to_array(img,resizing))\n",
    "        visited.append(stat(imagepath).st_size)\n",
    "    img.close()\n",
    "\n",
    "#extract shirts that aren't also jerseys with white background\n",
    "shirtsdir = listdir(\"51ShirtsJerseys/Shirts/\")\n",
    "shirts = []\n",
    "for image in shirtsdir:\n",
    "    imagepath = \"51ShirtsJerseys/Shirts/\" + image\n",
    "    if stat(imagepath).st_size not in visited:\n",
    "        img = Image.open(imagepath)\n",
    "        if (img.getpixel((0,0)) == (255,255,255)):\n",
    "            shirts.append(img_to_array(img,resizing))\n",
    "        img.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTlgVgamNpah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n",
      "372\n",
      "(5400,)\n"
     ]
    }
   ],
   "source": [
    "#OPTIONAL\n",
    "#take subset to optimize for testing execution speed\n",
    "#shirts = shirts[:100]\n",
    "#jerseys = jerseys[:100]\n",
    "print(len(shirts))\n",
    "print(len(jerseys))\n",
    "print(shirts[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQCEIa0H9hmH"
   },
   "outputs": [],
   "source": [
    "#extract data\n",
    "raw_data = [(row,'1') for row in jerseys] + [(row,'0') for row in shirts]\n",
    "X = np.array([x for (x,y) in raw_data])\n",
    "y = np.array([y for (x,y) in raw_data])\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "AQCYMG7aQXNS",
    "outputId": "d7bdb9ad-346f-4dc9-bb30-5af93d9b9092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   25.3s finished\n",
      "C:\\Users\\rabin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100} 0.6647058823529411\n",
      "Train Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       238\n",
      "           1       1.00      1.00      1.00       272\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       510\n",
      "   macro avg       1.00      1.00      1.00       510\n",
      "weighted avg       1.00      1.00      1.00       510\n",
      "\n",
      "Test Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.64        71\n",
      "           1       0.74      0.70      0.72       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       171\n",
      "   macro avg       0.68      0.68      0.68       171\n",
      "weighted avg       0.69      0.68      0.69       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear SVM for classification\n",
    "\n",
    "gs = GridSearchCV(LinearSVC(), {\"C\": [0.01,0.1,1,10,100]},cv=3, n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(\"Train Error:\")\n",
    "print(classification_report(y_train,gs.best_estimator_.predict(X_train)))\n",
    "print(\"Test Error:\")\n",
    "print(classification_report(y_test,gs.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7125fb40e4b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Check for explained variace over n_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "\n",
    "#Check for explained variace over n_components\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "plt.scatter(list(range(30)), pca.explained_variance_ratio_[:30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.67      0.63       238\n",
      "           1       0.67      0.58      0.62       272\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       510\n",
      "   macro avg       0.63      0.63      0.63       510\n",
      "weighted avg       0.63      0.63      0.63       510\n",
      "\n",
      "Test Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65        71\n",
      "           1       0.77      0.60      0.67       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       171\n",
      "   macro avg       0.67      0.67      0.66       171\n",
      "weighted avg       0.69      0.66      0.66       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Choosing 5 by elbow method\n",
    "pca=PCA(n_components=5)\n",
    "pca.fit(X_train)\n",
    "pca_train = pca.transform(X_train)\n",
    "SVMclf = LinearSVC()\n",
    "SVMclf.fit(pca_train, y_train)\n",
    "\n",
    "print(\"Train Error:\")\n",
    "print(classification_report(y_train,SVMclf.predict(pca_train)))\n",
    "print(\"Test Error:\")\n",
    "print(classification_report(y_test,SVMclf.predict(pca.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1elPfFFqQe-v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.01} 0.6490196078431373\n",
      "Train Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       238\n",
      "           1       0.95      0.96      0.96       272\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       510\n",
      "   macro avg       0.95      0.95      0.95       510\n",
      "weighted avg       0.95      0.95      0.95       510\n",
      "\n",
      "Test Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.62        71\n",
      "           1       0.73      0.74      0.74       100\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       171\n",
      "   macro avg       0.68      0.68      0.68       171\n",
      "weighted avg       0.69      0.69      0.69       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabin\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('lda', LDA(n_components=1)),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01,0.1,1,10,100],\n",
    "    },\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(pipe, cv=3, n_jobs=-1, param_grid=param_grid, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(\"Train Error:\")\n",
    "print(classification_report(y_train,gs.best_estimator_.predict(X_train)))\n",
    "print(\"Test Error:\")\n",
    "print(classification_report(y_test,gs.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.01} 0.6843137254901961\n",
      "Train Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67       238\n",
      "           1       0.72      0.66      0.69       272\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       510\n",
      "   macro avg       0.68      0.68      0.68       510\n",
      "weighted avg       0.68      0.68      0.68       510\n",
      "\n",
      "Test Error:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66        71\n",
      "           1       0.78      0.66      0.71       100\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       171\n",
      "   macro avg       0.69      0.70      0.69       171\n",
      "weighted avg       0.71      0.69      0.69       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    9.0s finished\n"
     ]
    }
   ],
   "source": [
    "#PCA and LDA\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('lda', LDA(n_components=1)),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01,0.1,1,10,100],\n",
    "    },\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(pipe, cv=3, n_jobs=-1, param_grid=param_grid, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(\"Train Error:\")\n",
    "print(classification_report(y_train,gs.best_estimator_.predict(X_train)))\n",
    "print(\"Test Error:\")\n",
    "print(classification_report(y_test,gs.best_estimator_.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Fqpd7Gt2mKH"
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Support Vector Machine is a supervised classifier, attempting to linearly separate the data while minimizing perpendicular distance between points and separator line. Using cross-validation to optimize a penalty parameter proved unfruitful, and the classifier kept on overfitting the data to an extreme amount, classifying correctly 100% of the training data. This is due to the dimensionality of our training data, having many more features than datapoints to train with. Reshaping the pictures to barely recognizable size was not helpful. Considering only pictures with white backgrounds, while scaling back our training set size, proved helpful because such photos are likely to be standardized and less features carry weight in the classification. This is where dimensionality reduction should then have a great effect.\n",
    "\n",
    "2. Principle Component Analysis is an unsupervised dimensionality reduction method, where data is projected down to lower dimensionality based on the directions of highest variance. Comparing the explained variance over many values for the number of final components, I've used the \"elbow method\" to pick the point where added dimensionality lead to significantly marginal returns. However, transforming the data and using SVM showed no improvement. However, training accuracy reduced without lowering test accuracy, showing signs of combating overfitting. I was hopeful that considering separability with LDA might improve on the classification accuracy.\n",
    "\n",
    "3. Linear Discriminant Analysis is a supervised dimensionality reduction method, where data is prohected down to lower dimensionality to maximize difference in means (separability). Sadly, no improvement was gained from LDA either. I took effort to eliminate duplicate data in the shirts and jerseys dataset, but that wasn't the factor limiting the efficacy of separability based classification.\n",
    "\n",
    "Combining approaches proved promising, and is my official recommendation even though significant improvement wasn't made. Also wear sweaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "CS156 Assignment 3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
